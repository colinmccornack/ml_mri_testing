{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9afa9908-21d0-4ebf-923c-117da5e62d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os\n",
    "import copy \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, Any, Optional\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.utils import save_image\n",
    "from IPython.display import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import transforms\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from torchvision import models\n",
    "from typing import List, Tuple, Optional, Dict, Any, Union\n",
    "# Import the model weights - takes time\n",
    "from torchvision.models import (\n",
    "    ResNet18_Weights, ResNet34_Weights, ResNet50_Weights, ResNet101_Weights, ResNet152_Weights,\n",
    "    EfficientNet_B0_Weights, EfficientNet_B1_Weights, EfficientNet_B2_Weights, EfficientNet_B3_Weights,\n",
    "    EfficientNet_B4_Weights, EfficientNet_B5_Weights, EfficientNet_B6_Weights, EfficientNet_B7_Weights,\n",
    "    EfficientNet_V2_S_Weights, EfficientNet_V2_M_Weights, EfficientNet_V2_L_Weights,\n",
    "    ConvNeXt_Tiny_Weights, ConvNeXt_Small_Weights, ConvNeXt_Base_Weights, ConvNeXt_Large_Weights,\n",
    "    DenseNet121_Weights, DenseNet161_Weights, DenseNet169_Weights, DenseNet201_Weights\n",
    ")\n",
    "\n",
    "# Seed function \n",
    "def seed_all(seed: int = 1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557ef1d8-879a-447e-9c60-7be65b480dd8",
   "metadata": {},
   "source": [
    "## Resources used: \n",
    "[CNN Cheatsheet from Stanford CS 230](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-convolutional-neural-networks)\n",
    "\n",
    "[Image classification: ResNet vs EfficientNet vs EfficientNet_v2 vs Compact Convolutional Transformers](https://medium.com/@enrico.randellini/image-classification-resnet-vs-efficientnet-vs-efficientnet-v2-vs-compact-convolutional-c205838bbf49). The code is essentially a generalized adaptation of this with some extra bells and whistles. \n",
    "\n",
    "[PyTorch docs](https://docs.pytorch.org/tutorials/). Self-explanatory\n",
    "\n",
    "Deepseek for plotting functions in Python and reformatting my poor python syntax. OOP is new to me so I had a lot of issues with initial attempts at this. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8dc52-3b9d-4a37-a029-98711f41eaaa",
   "metadata": {},
   "source": [
    "## Goal\n",
    "- The goal of this was to become familiar with OOP/python through an applied example of making a generalized transfer-learning workflow to apply existing CNN models to MRI image classification. I used two datasets for this, the first a collection of brain tumor MRIs to classify the malignancy, and the second a collection of Alzheimer's MRIs to classify disease severity ([found here](https://www.kaggle.com/datasets/alifatahi/multi-class-neurological-disorder-mcnd-dataset/data)). In exploring this, I quickly got lost in the weeds due to the complexity and diversity of approaches you can use to solve this classification problem and the current state of the field. In the end, I chose to ignore these, as the goal wasn't to make the most groundbreaking/novel approach to this problem (something I'm ill-equipped to do) but to learn how to work through it and structure code to do this using existing methods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301c249-3dc5-4b2f-9264-21a63e33f027",
   "metadata": {},
   "source": [
    "# Transformations for the training and test sets \n",
    "- This will be more important for other datasets/real data testing)\n",
    "- For the test dataset, use a simple transformation to resize and normalize the data before converting to a tensor\n",
    "- For the training dataset, use a more complex transformation that introduces shifts and transformations without grossly distorting biology\n",
    "- Note: Need to use ToTensorV2 at the end so they're the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2812bb4-3069-4ca6-ac14-81426843e74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_test = A.Compose([\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], seed = 1234)\n",
    "\n",
    "transform_train = A.Compose([\n",
    "    # Spatial transformations; flip, rotate, \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.Affine(scale=(0.9, 1.1), translate_percent=0.05, rotate=(-15, 15), p=0.5),\n",
    "    # Intensity, noise, and blur\n",
    "    A.GaussNoise(std_range=(0, 0.01), p=0.3),\n",
    "    A.Blur(blur_limit=3, p=0.2),\n",
    "    # : Elastic transformations \n",
    "    A.ElasticTransform(alpha=1, sigma=50, p=0.3),\n",
    "    # Normalize and convert to tensor\n",
    "    A.Resize(height=224, width=224),\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "], seed = 1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab2c791-ab76-4a1f-9f9b-44160c593217",
   "metadata": {},
   "source": [
    "## Dataset and Model structures for consistent inputs to the modeling\n",
    "### Dataset loaders\n",
    "- AlbumentationsDataset: Dataset class to wrap the Torch Imagefolder class and apply the transformations described above\n",
    "- create_datasets: Function that takes in a base directory (which itself contains the training, validation, and test directories) to feed into the albumentation transformations before returning a tuple of these datasets for modeling\n",
    "- create_dataloaders: Function to use the built-in Dataloader util to load the datasets from the base_dir and shuffle them, returning a tuple of the dataloader objects\n",
    "### Model config/loaders \n",
    "- ModelConfig: Class that holds various points of model/dataset information that are important, including\n",
    "    - Number of classes for classification\n",
    "    - Model name for lookup/comparison with the dictionary\n",
    "    - Number of nodes of the hidden layer\n",
    "    - Dropout rate in the classifier head\n",
    "    - Information about whether to freeze weights and how to unfreeze\n",
    "    - Whether to load ImageNet weights or not\n",
    "    - Whether to use the custom head or not\n",
    "- ImageClassificationModel: Class that does the following\n",
    "    - Holds a dictionary of usable models (excluding the ConvNeXt models for now)\n",
    "    - Validate the input model is in the dictionary\n",
    "    - Store information about the number of layers for unfreezing\n",
    "    - Defines the classifier head, which allows these pretrained models to be used for specific purposes of classifying our images of interest\n",
    "        - Individual classifier heads for the different built-in models\n",
    "        - Generalized classifier heads for other models that output 1D or 2D features\n",
    "    - Helper/debugging functions to troubleshoot model specific issues (mostly useful when using new models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "887b250d-6575-4c6e-b9c8-c92997d3eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a dataset class to hold the results of the albumentations transformation\n",
    "class AlbumentationsDataset(Dataset):\n",
    "    # 3 components to all dataset classes: _init_ (constructor), _len_ (gets length of the set), and _getitem_ (pulls individual instances from dataset)\n",
    "    def __init__(self, image_folder, transform=None):\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform\n",
    "\n",
    "        \n",
    "        # Get samples (image paths) and targets (labels) from the ImageFolder\n",
    "        self.samples = self.image_folder.samples\n",
    "        self.targets = self.image_folder.targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the path and label for the given index\n",
    "        path, label = self.samples[idx]\n",
    "        \n",
    "        # Load the image as a NumPy array (BGR) and convert to RGB\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Apply albumentations transforms \n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image)\n",
    "            image = augmented['image']\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "def create_datasets(base_dir: str) -> Tuple[Dataset, Dataset, Dataset]:\n",
    "    # Define the paths for each dataset\n",
    "    train_dir = os.path.join(base_dir, 'train')\n",
    "    test_dir = os.path.join(base_dir, 'test')\n",
    "    validation_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "    # Create ImageFolder datasets\n",
    "    base_train_dataset = ImageFolder(root=train_dir, transform=None)\n",
    "    base_test_dataset = ImageFolder(root=test_dir, transform=None)\n",
    "    base_val_dataset = ImageFolder(root=validation_dir, transform=None)\n",
    "\n",
    "    # Wrap the ImageFolder datasets with the custom AlbumentationsDataset from above\n",
    "    train_dataset = AlbumentationsDataset(\n",
    "        image_folder=base_train_dataset,\n",
    "        transform=transform_train\n",
    "    )\n",
    "    \n",
    "    test_dataset = AlbumentationsDataset(\n",
    "        image_folder=base_test_dataset,\n",
    "        transform=transform_test\n",
    "    )\n",
    "\n",
    "    # Apply the test transformation to the validation dataset \n",
    "    validation_dataset = AlbumentationsDataset(\n",
    "        image_folder=base_val_dataset,\n",
    "        transform=transform_test\n",
    "    )\n",
    "    \n",
    "    return train_dataset, test_dataset, validation_dataset\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "def create_dataloaders(base_dir: str, batch_size: int = 32, \n",
    "                      num_workers: int = 4, seed: int = 1234) -> Tuple[DataLoader, DataLoader, Optional[DataLoader], List[str]]:\n",
    "    # Set the seed \n",
    "    if seed is not None:\n",
    "        g = torch.Generator()\n",
    "        g.manual_seed(seed)\n",
    "    else:\n",
    "        g = None\n",
    "    \n",
    "    # Call the create_datasets function\n",
    "    train_dataset, test_dataset, val_dataset = create_datasets(base_dir)\n",
    "    \n",
    "    # Get class names from the dataset\n",
    "    # ImageFolder stores class names in .classes attribute\n",
    "    class_names = train_dataset.image_folder.classes\n",
    "    \n",
    "    # Define the DataLoader arguments\n",
    "    loader_args = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"num_workers\": num_workers,\n",
    "        \"pin_memory\": True, \n",
    "        \"generator\": g,\n",
    "        \"worker_init_fn\": seed_worker if seed is not None else None,\n",
    "    }\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, shuffle=True, **loader_args)\n",
    "    val_loader = DataLoader(val_dataset, shuffle=False, **loader_args) \n",
    "    \n",
    "    test_loader = None\n",
    "    if test_dataset:\n",
    "        test_loader = DataLoader(test_dataset, shuffle=False, **loader_args)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, class_names\n",
    "\n",
    "# Use the dataclass decorator so that the _init_ is generated for the ModelConfig class\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    num_classes: int\n",
    "    model_name: str\n",
    "    n_nodes: int = 512\n",
    "    dropout: float = 0.2\n",
    "    freeze_layers: bool = False\n",
    "    freeze_strategy: str = \"all\"\n",
    "    pretrained: bool = True\n",
    "    image_size: int = 224\n",
    "    custom_head: bool = True\n",
    "    debug: bool = False\n",
    "\n",
    "class ImageClassificationModel(nn.Module):\n",
    "    # Create a dictionary of the model configurations, using the default weights (could be turned into .yaml or external dictionary in the future)\n",
    "    MODEL_CONFIGS = {\n",
    "        # ResNet Models\n",
    "        'resnet18': {'base_model': models.resnet18, 'weights': ResNet18_Weights.DEFAULT, 'feature_dim': 512, 'classifier_attr': 'fc', 'pooling_attr': 'avgpool'},\n",
    "        'resnet34': {'base_model': models.resnet34, 'weights': ResNet34_Weights.DEFAULT, 'feature_dim': 512, 'classifier_attr': 'fc', 'pooling_attr': 'avgpool'},\n",
    "        'resnet50': {'base_model': models.resnet50, 'weights': ResNet50_Weights.DEFAULT, 'feature_dim': 2048, 'classifier_attr': 'fc', 'pooling_attr': 'avgpool'},\n",
    "        'resnet101': {'base_model': models.resnet101, 'weights': ResNet101_Weights.DEFAULT, 'feature_dim': 2048, 'classifier_attr': 'fc', 'pooling_attr': 'avgpool'},\n",
    "        'resnet152': {'base_model': models.resnet152, 'weights': ResNet152_Weights.DEFAULT, 'feature_dim': 2048, 'classifier_attr': 'fc', 'pooling_attr': 'avgpool'},\n",
    "        \n",
    "        # EfficientNet Models\n",
    "        'efficientnet_b0': {'base_model': models.efficientnet_b0, 'weights': EfficientNet_B0_Weights.DEFAULT, 'feature_dim': 1280, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b1': {'base_model': models.efficientnet_b1, 'weights': EfficientNet_B1_Weights.DEFAULT, 'feature_dim': 1280, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b2': {'base_model': models.efficientnet_b2, 'weights': EfficientNet_B2_Weights.DEFAULT, 'feature_dim': 1408, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b3': {'base_model': models.efficientnet_b3, 'weights': EfficientNet_B3_Weights.DEFAULT, 'feature_dim': 1536, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b4': {'base_model': models.efficientnet_b4, 'weights': EfficientNet_B4_Weights.DEFAULT, 'feature_dim': 1792, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b5': {'base_model': models.efficientnet_b5, 'weights': EfficientNet_B5_Weights.DEFAULT, 'feature_dim': 2048, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b6': {'base_model': models.efficientnet_b6, 'weights': EfficientNet_B6_Weights.DEFAULT, 'feature_dim': 2304, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_b7': {'base_model': models.efficientnet_b7, 'weights': EfficientNet_B7_Weights.DEFAULT, 'feature_dim': 2560, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_v2_s': {'base_model': models.efficientnet_v2_s, 'weights': EfficientNet_V2_S_Weights.DEFAULT, 'feature_dim': 1280, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_v2_m': {'base_model': models.efficientnet_v2_m, 'weights': EfficientNet_V2_M_Weights.DEFAULT, 'feature_dim': 1280, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        'efficientnet_v2_l': {'base_model': models.efficientnet_v2_l, 'weights': EfficientNet_V2_L_Weights.DEFAULT, 'feature_dim': 1280, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool'},\n",
    "        \n",
    "        # # ConvNeXt Models\n",
    "        # NOTE: As currently implemented with the custom classifier head, the ConvNeXt models don't work because of their architecture. Will work to fix but it isn't a priority \n",
    "        'convnext_tiny': {'base_model': models.convnext_tiny, 'weights': ConvNeXt_Tiny_Weights.DEFAULT, 'feature_dim': 768, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool', 'classifier_is_sequential': True},\n",
    "        'convnext_small': {'base_model': models.convnext_small, 'weights': ConvNeXt_Small_Weights.DEFAULT, 'feature_dim': 768, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool', 'classifier_is_sequential': True},\n",
    "        'convnext_base': {'base_model': models.convnext_base, 'weights': ConvNeXt_Base_Weights.DEFAULT, 'feature_dim': 1024, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool', 'classifier_is_sequential': True},\n",
    "        'convnext_large': {'base_model': models.convnext_large, 'weights': ConvNeXt_Large_Weights.DEFAULT, 'feature_dim': 1536, 'classifier_attr': 'classifier', 'pooling_attr': 'avgpool', 'classifier_is_sequential': True},\n",
    "        \n",
    "        # DenseNet Models\n",
    "        'densenet121': {'base_model': models.densenet121, 'weights': DenseNet121_Weights.DEFAULT, 'feature_dim': 1024, 'classifier_attr': 'classifier', 'pooling_attr': None, 'requires_pooling': False},\n",
    "        'densenet161': {'base_model': models.densenet161, 'weights': DenseNet161_Weights.DEFAULT, 'feature_dim': 2208, 'classifier_attr': 'classifier', 'pooling_attr': None, 'requires_pooling': False},\n",
    "        'densenet169': {'base_model': models.densenet169, 'weights': DenseNet169_Weights.DEFAULT, 'feature_dim': 1664, 'classifier_attr': 'classifier', 'pooling_attr': None, 'requires_pooling': False},\n",
    "        'densenet201': {'base_model': models.densenet201, 'weights': DenseNet201_Weights.DEFAULT, 'feature_dim': 1920, 'classifier_attr': 'classifier', 'pooling_attr': None, 'requires_pooling': False}\n",
    "    }\n",
    "\n",
    "    def __init__(self, cfg: Dict[str, Any]):\n",
    "        # Use super() to extend the behavior of the parent constructor\n",
    "        super().__init__()\n",
    "        # Create the \n",
    "        self.config = self._create_config(cfg)\n",
    "        self.model = self.get_model(self.config.model_name)\n",
    "        \n",
    "        # Store layer information for unfreezing\n",
    "        self._setup_layer_tracking()\n",
    "        \n",
    "        if self.config.debug:\n",
    "            self._debug_info()\n",
    "\n",
    "    # Validate the input configuration and create the ModelConfig object\n",
    "    def _create_config(self, cfg_dict: dict) -> ModelConfig:\n",
    "        # Check for the required inputs (name and number of classes) \n",
    "        required = [\"num_classes\", \"model_name\"]\n",
    "        model_cfg = cfg_dict.get(\"model\", {})\n",
    "        \n",
    "        for key in required:\n",
    "            if key not in model_cfg:\n",
    "                raise ValueError(f\"Missing required config key: model.{key}\")\n",
    "        \n",
    "        data_cfg = cfg_dict.get(\"data\", {})\n",
    "\n",
    "        # Create the ModelConfig w/ defaults \n",
    "        return ModelConfig(\n",
    "            num_classes=model_cfg[\"num_classes\"],\n",
    "            model_name=model_cfg[\"model_name\"],\n",
    "            n_nodes=model_cfg.get(\"n_nodes\", 512),\n",
    "            dropout=model_cfg.get(\"dropout\", 0.2),\n",
    "            freeze_layers=model_cfg.get(\"freeze_layers\", False),\n",
    "            freeze_strategy=model_cfg.get(\"freeze_strategy\", \"all\"),\n",
    "            pretrained=model_cfg.get(\"pretrained\", True),\n",
    "            image_size=data_cfg.get(\"size\", 224),\n",
    "            custom_head=model_cfg.get(\"custom_head\", True),\n",
    "            debug=cfg_dict.get(\"debug\", False)\n",
    "        )\n",
    "\n",
    "    # Store the layer names for unfreezing later \n",
    "    def _setup_layer_tracking(self):\n",
    "        self.layer_names = []\n",
    "        self.param_layers = []\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            self.layer_names.append(name)\n",
    "            self.param_layers.append(param)\n",
    "        \n",
    "        # Store total layers count for unfreezing strategies\n",
    "        self.total_layers = len(self.param_layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def freeze_layers_base_model(self, model):\n",
    "        for param in model.parameters():\n",
    "            # Remove the gradient requirement to freeze the layer \n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_layers(self, layer_start_unfreeze: Optional[int] = None):\n",
    "        if layer_start_unfreeze is not None:\n",
    "            print(f\"Unfreezing layers starting from index {layer_start_unfreeze}\")\n",
    "            for i, param in enumerate(self.param_layers):\n",
    "                if i >= layer_start_unfreeze:\n",
    "                    param.requires_grad = True\n",
    "                    if self.config.debug:\n",
    "                        print(f\"Unfrozen: {self.layer_names[i]}\")\n",
    "        else:\n",
    "            # Unfreeze all layers\n",
    "            print(\"Unfreezing all layers\")\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = True\n",
    "\n",
    "    def get_trainable_parameters_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "\n",
    "    def get_model(self, name_pretrained_model: str) -> nn.Module:\n",
    "        # Check if the model name is in the dictonary above \n",
    "        if name_pretrained_model not in self.MODEL_CONFIGS:\n",
    "            raise ValueError(f\"Unsupported model: {name_pretrained_model}\")\n",
    "        \n",
    "        config = self.MODEL_CONFIGS[name_pretrained_model]\n",
    "        \n",
    "        # Load pretrained model\n",
    "        if self.config.pretrained:\n",
    "            base_model = config['base_model'](weights=config['weights'])\n",
    "        else:\n",
    "            base_model = config['base_model'](weights=None)\n",
    "        \n",
    "        # Freeze layers if configured\n",
    "        if self.config.freeze_layers:\n",
    "            print(\"Freezing layers of pretrained model\")\n",
    "            self.freeze_layers_base_model(base_model)\n",
    "        \n",
    "        # Replace the classifier head based on architecture\n",
    "        classifier_attr = config['classifier_attr']\n",
    "        \n",
    "        # Match the input model with the heads defined below \n",
    "        if name_pretrained_model.startswith('resnet'):\n",
    "            custom_head = self._create_resnet_head(config['feature_dim'])\n",
    "        elif name_pretrained_model.startswith('efficientnet'):\n",
    "            custom_head = self._create_efficientnet_head(config['feature_dim'])\n",
    "        elif name_pretrained_model.startswith('convnext'):\n",
    "            custom_head = self._create_convnext_head(config['feature_dim'])\n",
    "        elif name_pretrained_model.startswith('densenet'):\n",
    "            custom_head = self._create_densenet_head(config['feature_dim'])\n",
    "        else:\n",
    "            # Default to 1D head for other models (can change if needed) \n",
    "            custom_head = self._create_1d_classifier_head(config['feature_dim'])\n",
    "\n",
    "        # Set the final layer of the models to be the custom head using setattr, corresponding to the layers stored in the dictionary \n",
    "        setattr(base_model, classifier_attr, custom_head)\n",
    "        return base_model\n",
    "\n",
    "    # ResNet, EfficientNet, and DenseNet all do global average pooling so the custom heads are just linear layers + ReLU\n",
    "    # In theory these could all be the same, but I was getting errors when I had one head for these models and one head for ConvNeXt\n",
    "    # I think it was because the old version didn't have the setattr above so the ResNet final layers weren't being replace properly with the custom head \n",
    "    def _create_resnet_head(self, feature_dim: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(feature_dim, self.config.n_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout),\n",
    "            nn.Linear(self.config.n_nodes, self.config.num_classes)\n",
    "        )\n",
    "    \n",
    "    def _create_efficientnet_head(self, feature_dim: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(feature_dim, self.config.n_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout),\n",
    "            nn.Linear(self.config.n_nodes, self.config.num_classes)\n",
    "        )\n",
    "        \n",
    "    def _create_densenet_head(self, feature_dim: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(feature_dim, self.config.n_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout),\n",
    "            nn.Linear(self.config.n_nodes, self.config.num_classes)\n",
    "        )\n",
    "    \n",
    "    # ConvNeXt outputs a 4D tensor w/ batch, channels, height, and width, so it requires pooling and flattening before going to linear layers \n",
    "    def _create_convnext_head(self, feature_dim: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.LayerNorm(feature_dim, eps=1e-6),\n",
    "            nn.Linear(feature_dim, self.config.n_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout),\n",
    "            nn.Linear(self.config.n_nodes, self.config.num_classes)\n",
    "        )\n",
    "    \n",
    "    # General 1D head (for models that output 1D features)\n",
    "    def _create_1d_classifier_head(self, feature_dim: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(feature_dim, self.config.n_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout),\n",
    "            nn.Linear(self.config.n_nodes, self.config.num_classes)\n",
    "        )\n",
    "    \n",
    "    # General 2D head (for models that need pooling)\n",
    "    def _create_2d_classifier_head(self, feature_dim: int) -> nn.Sequential:\n",
    "        return nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(feature_dim, self.config.n_nodes),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.config.dropout),\n",
    "            nn.Linear(self.config.n_nodes, self.config.num_classes)\n",
    "        )\n",
    "\n",
    "    # General model info debugging \n",
    "    def _debug_info(self):\n",
    "        print(f\"\\n=== Model Configuration ===\")\n",
    "        print(f\"Architecture: {self.config.model_name}\")\n",
    "        print(f\"Num classes: {self.config.num_classes}\")\n",
    "        print(f\"Hidden layer size: {self.config.n_nodes}\")\n",
    "        print(f\"Dropout: {self.config.dropout}\")\n",
    "        print(f\"Frozen layers: {self.config.freeze_layers}\")\n",
    "        \n",
    "        total_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = self.get_trainable_parameters_count()\n",
    "        print(f\"Total parameters: {total_params:,}\")\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"Total layers: {self.total_layers}\")\n",
    "        print(\"=\" * 30)\n",
    "\n",
    "    # Model structure debugging (AI written when I was having trouble with the custom heads above d/t the naming convention of ResNet's final layer \n",
    "    def debug_model_structure(self, model_name: str):\n",
    "        config = self.MODEL_CONFIGS[model_name]\n",
    "        \n",
    "        if self.config.pretrained:\n",
    "            base_model = config['base_model'](weights=config['weights'])\n",
    "        else:\n",
    "            base_model = config['base_model'](weights=None)\n",
    "        \n",
    "        print(f\"\\n=== Debugging {model_name} ===\")\n",
    "        print(f\"Classifier attribute: {config['classifier_attr']}\")\n",
    "        \n",
    "        classifier = getattr(base_model, config['classifier_attr'])\n",
    "        print(f\"Classifier type: {type(classifier)}\")\n",
    "        print(f\"Classifier structure: {classifier}\")\n",
    "        \n",
    "        # Check if it's sequential\n",
    "        if isinstance(classifier, nn.Sequential):\n",
    "            print(\"\\nSequential layers:\")\n",
    "            for i, layer in enumerate(classifier):\n",
    "                print(f\"  [{i}] {layer.__class__.__name__}: {layer}\")\n",
    "        \n",
    "        print(f\"\\nFeature dimension: {config['feature_dim']}\")\n",
    "        print(\"=\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739bccb-6a34-4203-a945-3d819638e051",
   "metadata": {},
   "source": [
    "## Definitions and classes below are components of the transfer learning function, called by train_model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246744d9-6af1-42b7-9ca1-8990f063b276",
   "metadata": {},
   "source": [
    "### EarlyStopping\n",
    "- The EarlyStopping class helps to prevent overfitting in the model by monitoring the validation loss for each epoch and ending the training if the validation loss increases\n",
    "- This prevents the model from learning from noise within the training set and overfitting to the training set \n",
    "- This improves generalizability and performance on external data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d68d451-6204-4cb4-bbd2-a54fcf6bde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \n",
    "    # Set the defaults, most importantly the counter and the overall patience \n",
    "    def __init__(self, patience=7, min_delta=0, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        self.best_weights = None\n",
    "\n",
    "    # Set the best loss to the validation loss if not present, and check of the validation loss for the current epoch is greater than the greatest loss so far \n",
    "    def __call__(self, val_loss, model=None):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "            if model and self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy()\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "                if model and self.restore_best_weights and self.best_weights:\n",
    "                    model.load_state_dict(self.best_weights)\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if model and self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8403f150-72f7-4502-a4fb-01086df7ee53",
   "metadata": {},
   "source": [
    "### TrainingMetrics \n",
    "- The TrainingMetrics class stores information about the accuracy, loss, learning rate, and precision/recall for a given epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "491e434e-c68e-4f55-b6a5-2fdbe4b9ec75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingMetrics:\n",
    "    def __init__(self):\n",
    "        # Basic metrics\n",
    "        self.train_losses = []\n",
    "        self.train_accuracies = []\n",
    "        self.val_losses = []\n",
    "        self.val_accuracies = []\n",
    "        self.learning_rates = []\n",
    "        self.best_accuracy = 0.0\n",
    "        \n",
    "        # Comprehensive metrics\n",
    "        self.val_auroc_macro = []\n",
    "        self.val_auroc_micro = []\n",
    "        self.val_f1_macro = []\n",
    "        self.val_f1_micro = []\n",
    "        self.val_precision_macro = []\n",
    "        self.val_recall_macro = []\n",
    "        self.val_f1_per_class = []  # List of dictionaries\n",
    "        \n",
    "        # Best epoch storage\n",
    "        self.best_epoch = -1\n",
    "    \n",
    "    def update(self, train_loss, train_acc, val_loss, val_acc, lr,\n",
    "               auroc_macro=None, auroc_micro=None,\n",
    "               f1_macro=None, f1_micro=None,\n",
    "               precision_macro=None, recall_macro=None,\n",
    "               f1_per_class=None):\n",
    "        self.train_losses.append(train_loss)\n",
    "        self.train_accuracies.append(train_acc)\n",
    "        self.val_losses.append(val_loss)\n",
    "        self.val_accuracies.append(val_acc)\n",
    "        self.learning_rates.append(lr)\n",
    "        \n",
    "        if auroc_macro is not None:\n",
    "            self.val_auroc_macro.append(auroc_macro)\n",
    "        if auroc_micro is not None:\n",
    "            self.val_auroc_micro.append(auroc_micro)\n",
    "        if f1_macro is not None:\n",
    "            self.val_f1_macro.append(f1_macro)\n",
    "        if f1_micro is not None:\n",
    "            self.val_f1_micro.append(f1_micro)\n",
    "        if precision_macro is not None:\n",
    "            self.val_precision_macro.append(precision_macro)\n",
    "        if recall_macro is not None:\n",
    "            self.val_recall_macro.append(recall_macro)\n",
    "        if f1_per_class is not None:  # Fixed parameter name\n",
    "            self.val_f1_per_class.append(f1_per_class)\n",
    "        \n",
    "        # Update best accuracy and epoch\n",
    "        if val_acc > self.best_accuracy:\n",
    "            self.best_accuracy = val_acc\n",
    "            self.best_epoch = len(self.val_accuracies) - 1  # Current epoch index\n",
    "    \n",
    "    def get_summary(self) -> Dict[str, float]:\n",
    "        # Return the best/final values or 0 if not run \n",
    "        return {\n",
    "            'best_val_accuracy': self.best_accuracy,\n",
    "            'final_train_loss': self.train_losses[-1] if self.train_losses else 0,\n",
    "            'final_val_loss': self.val_losses[-1] if self.val_losses else 0,\n",
    "            'final_train_acc': self.train_accuracies[-1] if self.train_accuracies else 0,\n",
    "            'final_val_acc': self.val_accuracies[-1] if self.val_accuracies else 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310cfe98-5624-4ac0-bc3a-7205b7ba31b2",
   "metadata": {},
   "source": [
    "### unfreeze_model_layers\n",
    "- By default the model layers are all unfrozen. When we run the training function we will freeze all of the model layers, which reduces the computational cost that would come from training each of the individual layers of all of the models\n",
    "- When we freeze the layers, the weights won't be updated during the backpropagation step, meaning the representations already trained are being used\n",
    "- Since we're tweaking image recognition models to a specific task, this makes sense because it allows us to fine tune the final layers (which are more task-specific) while preserving the initial layers which contain foundational image features like texture, edges, etc.\n",
    "- In the future, this could be improved by being more specific with the conditional freezing using pytorch hooks on specific layers to freeze/unfreeze if the gradient magnitude changes by a certain threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6243d4d-cd21-40d5-8cfe-947c86eab086",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_model_layers(model, epoch_start_unfreeze, layer_start_unfreeze, current_epoch):\n",
    "    \n",
    "    # Check if the model is frozen or not, and if the current epoch is when we would unfreeze if the model is frozen \n",
    "    if epoch_start_unfreeze is not None and current_epoch >= epoch_start_unfreeze:\n",
    "        print(\"Unfreezing base model weights\")\n",
    "\n",
    "        # Check if we're unfreezing selective layers or the whole thing by changing how PyTorch is monitoring the gradient flow (param.requires_grad)\n",
    "        if layer_start_unfreeze is not None:\n",
    "            print(f\"Unfreezing layers >= {layer_start_unfreeze}\")\n",
    "            # Unfreeze only layers â‰¥ layer_start_unfreeze\n",
    "            for i, (name, param) in enumerate(model.named_parameters()):\n",
    "                if i >= layer_start_unfreeze:\n",
    "                    param.requires_grad = True\n",
    "        else:\n",
    "            # Unfreeze all layers\n",
    "            print(\"Unfreezing all model layers\")\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = True\n",
    "                \n",
    "        return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dd9e30-6d67-402b-921e-72e44f0d9466",
   "metadata": {},
   "source": [
    "### save_checkpoint\n",
    "- Straightforward, saves model information/parameters through the epochs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "788a6c8f-5554-4c18-8e53-62d532462184",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, scheduler, epoch, metrics, is_best, checkpoint_dir):\n",
    "    \n",
    "    checkpoint = {\n",
    "        # Save the epoch number \n",
    "        'epoch': epoch,\n",
    "        # Save the model state (stored in the state_dict), which contains infor about both the model and optimizer \n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "        # Save model metadata \n",
    "        'model_metadata': {\n",
    "            'model_name': model.config.model_name,\n",
    "            'num_classes': model.config.num_classes,\n",
    "            'feature_dim': getattr(model, 'feature_dim', 'N/A'),\n",
    "            'total_params': sum(p.numel() for p in model.parameters()),\n",
    "            'trainable_params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "        },\n",
    "        # Save information about the model performance \n",
    "        'metrics': {\n",
    "            'best_accuracy': metrics.best_accuracy,\n",
    "            'current_val_accuracy': metrics.val_accuracies[-1] if metrics.val_accuracies else 0,\n",
    "            'current_train_accuracy': metrics.train_accuracies[-1] if metrics.train_accuracies else 0,\n",
    "        },\n",
    "        # Save information about the model training (total epochs, early stopping)\n",
    "        'training_info': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'total_epochs': len(metrics.train_losses),\n",
    "            'early_stopped': len(metrics.train_losses) < getattr(model.config, 'num_epochs', 100)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save latest model \n",
    "    latest_path = os.path.join(checkpoint_dir, 'latest.pth')\n",
    "    torch.save(checkpoint, latest_path)\n",
    "    \n",
    "    # Save best if current epoch is the best \n",
    "    if is_best:\n",
    "        best_path = os.path.join(checkpoint_dir, 'best.pth')\n",
    "        torch.save(checkpoint, best_path)\n",
    "        print(f\"Best model updated and saved to: {best_path}\")\n",
    "    \n",
    "    return latest_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a4becf-6868-4fdf-847a-25b48149b902",
   "metadata": {},
   "source": [
    "### train_epoch\n",
    "- Single training epoch function, returns the loss and accuracy of that individual epoch\n",
    "- For each batch in the dataloader, do the following \n",
    "  - Move batch to device\n",
    "  - Zero gradients (optimizer.zero_grad())\n",
    "  - Forward pass (model(inputs))\n",
    "  - Compute loss (criterion(outputs, targets))\n",
    "  - Backward pass (loss.backward())\n",
    "  - Update weights (optimizer.step())\n",
    "  - Accumulate batch metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "787e5717-93f7-4f51-83af-9ce20fd0072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    \n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Make a progress bar with tqdm to make it pretty \n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
    "    for inputs, labels in progress_bar:\n",
    "        # Load the data to the CPU/GPU (if I'm lucky enough to have one available)\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Clear the gradients so they dopn't accumulate in buffers\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass through the model \n",
    "        outputs = model(inputs)\n",
    "        # Calculate the loss (CrossEntropyLoss)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        # Calculate the gradient\n",
    "        loss.backward()\n",
    "        # Update the model parameters if unfrozen\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy by taking the correct outputs and summing \n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct = (preds == labels).sum().item()\n",
    "        \n",
    "        # Update running metrics by summing the total losses\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += correct\n",
    "        total_samples += inputs.size(0)\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({\n",
    "            'Loss': f'{loss.item():.4f}',\n",
    "            'Acc': f'{correct/inputs.size(0):.4f}'\n",
    "        })\n",
    "\n",
    "    # Calcualte the epoch-wide weighted averages\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = running_corrects / total_samples\n",
    "    \n",
    "    return epoch_loss, epoch_acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c851d573-840d-48e9-99da-8c300c2d9398",
   "metadata": {},
   "source": [
    "### validate_epoch_with_metrics\n",
    "- Similar to the training step, but is just a forward pass through the model without updating neuron weights for the validation step\n",
    "- Also stores information about the validation accuracy for given epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97ee288b-85e1-451c-a267-4169ce01ce28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_epoch_with_metrics(model, dataloader, criterion, device, class_names=None):\n",
    "    model.eval()\n",
    "    total_loss, total_correct = 0, 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Store all predictions for metrics computation\n",
    "    all_true_labels = []\n",
    "    all_pred_probs = []\n",
    "    all_pred_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Get probabilities and predictions\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            total_correct += preds.eq(targets).sum().item()\n",
    "            total_samples += inputs.size(0)\n",
    "            \n",
    "            # Store for metrics computation\n",
    "            all_true_labels.append(targets.cpu())\n",
    "            all_pred_probs.append(probs.cpu())\n",
    "            all_pred_labels.append(preds.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_true = torch.cat(all_true_labels, dim=0).numpy()\n",
    "    all_probs = torch.cat(all_pred_probs, dim=0).numpy()\n",
    "    all_preds = torch.cat(all_pred_labels, dim=0).numpy()\n",
    "    \n",
    "    # Compute basic metrics\n",
    "    avg_loss = total_loss / total_samples\n",
    "    accuracy = total_correct / total_samples\n",
    "    \n",
    "    # Compute class-specific (AUROC) if class_names provided using the helper function below\n",
    "    comp_metrics = {}\n",
    "    if class_names is not None:\n",
    "        comp_metrics = compute_comprehensive_metrics(\n",
    "            all_true, all_probs, all_preds, class_names\n",
    "        )\n",
    "    \n",
    "    return avg_loss, accuracy, comp_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff8a86-fc30-4eb7-b20a-974768bb8991",
   "metadata": {},
   "source": [
    "### compute_comprehensive_metrics \n",
    "- Compute and store metrics of the model performance, such as AUROC, F1 scores, precision and recall, and per-class probability "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e4065b5-a6b7-4421-a9b0-06fc77e1a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_comprehensive_metrics(true_labels, pred_probs, pred_labels, class_names):\n",
    "    # import sklearn when running\n",
    "    from sklearn.metrics import (roc_auc_score, f1_score, precision_score, \n",
    "                                 recall_score, confusion_matrix)\n",
    "    # Pull the total number of classes and instantiate a dictionary to hold the metrics \n",
    "    n_classes = len(class_names)\n",
    "    metrics = {}\n",
    "\n",
    "    # Use sklearn functions for calculations of AUROC and F1 scores \n",
    "    # AUROC calculation\n",
    "    # Binary classes \n",
    "    if n_classes == 2:\n",
    "        metrics['auroc_macro'] = roc_auc_score(true_labels, pred_probs[:, 1])\n",
    "        metrics['auroc_micro'] = metrics['auroc_macro']\n",
    "    # Multi-class AUROC\n",
    "    else:\n",
    "        from sklearn.preprocessing import label_binarize\n",
    "        true_binary = label_binarize(true_labels, classes=range(n_classes))\n",
    "        # Average of per-class AUROCs\n",
    "        metrics['auroc_macro'] = roc_auc_score(true_binary, pred_probs, \n",
    "                                              average='macro', multi_class='ovr')\n",
    "        # One-versus-all binarization AUROC\n",
    "        metrics['auroc_micro'] = roc_auc_score(true_binary, pred_probs, \n",
    "                                              average='micro', multi_class='ovr')\n",
    "    \n",
    "    # F1 Scores\n",
    "    metrics['f1_macro'] = f1_score(true_labels, pred_labels, average='macro')\n",
    "    metrics['f1_micro'] = f1_score(true_labels, pred_labels, average='micro')\n",
    "    \n",
    "    # Precision and Recall\n",
    "    metrics['precision_macro'] = precision_score(true_labels, pred_labels, average='macro')\n",
    "    metrics['recall_macro'] = recall_score(true_labels, pred_labels, average='macro')\n",
    "    \n",
    "    # Per-class metrics\n",
    "    metrics['per_class_f1'] = dict(zip(class_names, \n",
    "        f1_score(true_labels, pred_labels, average=None)))\n",
    "    metrics['per_class_precision'] = dict(zip(class_names,\n",
    "        precision_score(true_labels, pred_labels, average=None)))\n",
    "    metrics['per_class_recall'] = dict(zip(class_names,\n",
    "        recall_score(true_labels, pred_labels, average=None)))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    metrics['confusion_matrix'] = confusion_matrix(true_labels, pred_labels)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f18b237-2362-4b23-ba53-f9c541df75c9",
   "metadata": {},
   "source": [
    "### plot_learning_curves\n",
    "- Plot the learning curves for the epochs (done every 5 epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90d82838-be7c-413d-8426-9fdee5967dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curves(epochs, metrics, save_path):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss curves\n",
    "    ax1.plot(range(1, epochs + 1), metrics.train_losses, label='Training Loss')\n",
    "    ax1.plot(range(1, epochs + 1), metrics.val_losses, label='Validation Loss')\n",
    "    ax1.set_title('Loss Curves')\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    # Accuracy curves\n",
    "    ax2.plot(range(1, epochs + 1), metrics.train_accuracies, label='Training Accuracy')\n",
    "    ax2.plot(range(1, epochs + 1), metrics.val_accuracies, label='Validation Accuracy')\n",
    "    ax2.set_title('Accuracy Curves')\n",
    "    ax2.set_xlabel('Epochs')\n",
    "    ax2.set_ylabel('Accuracy')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    # Learning rate\n",
    "    ax3.plot(range(1, epochs + 1), metrics.learning_rates, label='Learning Rate', color='red')\n",
    "    ax3.set_title('Learning Rate Schedule')\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Learning Rate')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True)\n",
    "    ax3.set_yscale('log')\n",
    "    \n",
    "    # Combined plot\n",
    "    ax4.plot(metrics.train_losses, metrics.train_accuracies, 'o-', label='Training', alpha=0.7)\n",
    "    ax4.plot(metrics.val_losses, metrics.val_accuracies, 'o-', label='Validation', alpha=0.7)\n",
    "    ax4.set_title('Loss vs Accuracy')\n",
    "    ax4.set_xlabel('Loss')\n",
    "    ax4.set_ylabel('Accuracy')\n",
    "    ax4.legend()\n",
    "    ax4.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"Learning curves saved to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3766ca88-62db-4121-ba79-dba25f7d0e52",
   "metadata": {},
   "source": [
    "### plot_metrics\n",
    "- Plot the evaluation metrics of the model performance, display, and save to file \n",
    "  - Loss curves\n",
    "  - Accuracy curves\n",
    "  - AUROC curves\n",
    "  - F1 score curves\n",
    "  - Precision-Recall curves\n",
    "  - Learning rate (mostly unnecessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b26b17ac-1db4-40a3-9fb8-768e4eb94439",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics, class_names, save_path=None):\n",
    "    n_epochs = len(metrics.val_losses)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # 1. Loss Curves\n",
    "    axes[0].plot(range(1, n_epochs + 1), metrics.train_losses, label='Train')\n",
    "    axes[0].plot(range(1, n_epochs + 1), metrics.val_losses, label='Val')\n",
    "    axes[0].set_title('Loss Curves')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Accuracy Curves\n",
    "    axes[1].plot(range(1, n_epochs + 1), metrics.train_accuracies, label='Train')\n",
    "    axes[1].plot(range(1, n_epochs + 1), metrics.val_accuracies, label='Val')\n",
    "    axes[1].axhline(y=metrics.best_accuracy, color='r', linestyle='--', \n",
    "                   label=f'Best: {metrics.best_accuracy:.4f}')\n",
    "    axes[1].set_title('Accuracy Curves')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. AUROC Curves\n",
    "    if metrics.val_auroc_macro and len(metrics.val_auroc_macro) == n_epochs:\n",
    "        axes[2].plot(range(1, n_epochs + 1), metrics.val_auroc_macro, label='Macro')\n",
    "        axes[2].plot(range(1, n_epochs + 1), metrics.val_auroc_micro, label='Micro')\n",
    "        axes[2].set_title('Validation AUROC')\n",
    "        axes[2].set_xlabel('Epoch')\n",
    "        axes[2].set_ylabel('AUROC')\n",
    "        axes[2].legend()\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 4. F1 Score Curves\n",
    "    if metrics.val_f1_macro and len(metrics.val_f1_macro) == n_epochs:\n",
    "        axes[3].plot(range(1, n_epochs + 1), metrics.val_f1_macro, label='Macro')\n",
    "        axes[3].plot(range(1, n_epochs + 1), metrics.val_f1_micro, label='Micro')\n",
    "        axes[3].set_title('Validation F1 Score')\n",
    "        axes[3].set_xlabel('Epoch')\n",
    "        axes[3].set_ylabel('F1 Score')\n",
    "        axes[3].legend()\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Precision-Recall Curves\n",
    "    if (metrics.val_precision_macro and metrics.val_recall_macro and\n",
    "        len(metrics.val_precision_macro) == n_epochs):\n",
    "        axes[4].plot(range(1, n_epochs + 1), metrics.val_precision_macro, label='Precision')\n",
    "        axes[4].plot(range(1, n_epochs + 1), metrics.val_recall_macro, label='Recall')\n",
    "        axes[4].set_title('Validation Precision & Recall (Macro)')\n",
    "        axes[4].set_xlabel('Epoch')\n",
    "        axes[4].set_ylabel('Score')\n",
    "        axes[4].legend()\n",
    "        axes[4].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. Learning Rate\n",
    "    axes[5].plot(range(1, n_epochs + 1), metrics.learning_rates, color='red')\n",
    "    axes[5].set_title('Learning Rate Schedule')\n",
    "    axes[5].set_xlabel('Epoch')\n",
    "    axes[5].set_ylabel('Learning Rate')\n",
    "    axes[5].set_yscale('log')\n",
    "    axes[5].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Hide unused subplots (if any)\n",
    "    for i in range(6, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5910543-b4b3-4ab2-8e54-bc682151c0f9",
   "metadata": {},
   "source": [
    "### train_model\n",
    "- The actual training function\n",
    "- Initializes metrics for storing model and validation information (validation loss)\n",
    "- Move the model to the GPU and set the freeze state\n",
    "- Run the individual epoch steps\n",
    "  - Print the epoch number\n",
    "  - Check the epoch number and unfreeze if the number is higher than the unfreeze starting number\n",
    "    - For unfrozen steps, recreate the optimizer and preserve the learning rate. Can adjust in the future to have different learning rates for the classifier and base layers of the model\n",
    "  - Run train_epoch (described above)\n",
    "  - Run validate_epoch (also described above)\n",
    "  - Get the learning rate and store\n",
    "  - Update the best accuracy parameter if the validation accuracy is higher for this epoch\n",
    "  - Print information about the performance on the training and validation for this epoch\n",
    "  - Check the validation loss and increase the early stopping value if there is no improvement "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44b88db5-81d6-4cba-a84d-19f447379b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    device, model, criterion, optimizer, lr_scheduler,\n",
    "    train_dataloader, val_dataloader, num_epochs, checkpoint_dir,\n",
    "    epoch_start_unfreeze=None, layer_start_unfreeze=None,\n",
    "    early_stopping_patience=10, resume_from_checkpoint=None, class_names=None, seed = None):\n",
    "\n",
    "    # Initialize the seed value\n",
    "    if seed is not None:\n",
    "        seed_all(seed)\n",
    "        print(f\"Seed val: {seed}\")\n",
    "    \n",
    "    # Initialize metrics and early stopping\n",
    "    metrics = TrainingMetrics()\n",
    "    early_stopper = EarlyStopping(patience=early_stopping_patience)\n",
    "    \n",
    "    # Set the starting epoch to 0\n",
    "    start_epoch = 0\n",
    "    # If the resume_from_checkpoint flag is set, resume from a previous checkpoint\n",
    "    if resume_from_checkpoint and os.path.exists(resume_from_checkpoint):\n",
    "        print(f\"Loading checkpoint: {resume_from_checkpoint}\")\n",
    "        # Load the saved .pth file \n",
    "        checkpoint = torch.load(resume_from_checkpoint, map_location=device)\n",
    "        # Load all the model weights and the optimizer state\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        # Check the learning rate scheduler and set \n",
    "        if lr_scheduler and 'scheduler_state_dict' in checkpoint:\n",
    "            lr_scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        # Adjust the value of the starting epoch to the last value + 1\n",
    "        start_epoch = checkpoint['epoch'] + 1\n",
    "        # Load metrics if available\n",
    "        if 'metrics' in checkpoint:\n",
    "            metrics = checkpoint['metrics']\n",
    "        print(f\"Resumed training from epoch {start_epoch}\")\n",
    "\n",
    "    # Send the model to the CPU/GPU\n",
    "    model = model.to(device)\n",
    "    # Start in the frozen state \n",
    "    freezed = model.config.freeze_layers\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        # Decorative formatting fo rthe epoch output\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Handle layer unfreezing using model's built-in method\n",
    "        if freezed and epoch_start_unfreeze is not None and epoch >= epoch_start_unfreeze:\n",
    "            model.unfreeze_layers(layer_start_unfreeze)\n",
    "            freezed = False\n",
    "            \n",
    "            # Recreate optimizer to include newly unfrozen parameters\n",
    "            if layer_start_unfreeze is not None:\n",
    "                print(\"Recreating optimizer with newly unfrozen parameters\")\n",
    "                # Make a list of all the parameters that currently require gradients (i.e. unfrozen)\n",
    "                trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "                # Create a new Adam optimizer for the parameters using the learning rate of the previous optimizer\n",
    "                optimizer = optim.Adam(trainable_params, lr=optimizer.param_groups[0]['lr'])\n",
    "        \n",
    "        # Training phase: Call train_epoch on the \n",
    "        train_loss, train_acc = train_epoch(model, train_dataloader, criterion, \n",
    "                                          optimizer, device)\n",
    "        \n",
    "        # Validation phase with metrics storage \n",
    "        if class_names is not None:\n",
    "            val_loss, val_acc, comp_metrics = validate_epoch_with_metrics(\n",
    "                model, val_dataloader, criterion, device, class_names\n",
    "            )\n",
    "        else:\n",
    "            # Fallback to basic validation\n",
    "            val_loss, val_acc = validate_epoch(model, val_dataloader, criterion, device)\n",
    "            comp_metrics = {}\n",
    "        \n",
    "        # Get current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics.update(\n",
    "            train_loss, train_acc, val_loss, val_acc, current_lr,\n",
    "            auroc_macro=comp_metrics.get('auroc_macro'),\n",
    "            auroc_micro=comp_metrics.get('auroc_micro'),\n",
    "            f1_macro=comp_metrics.get('f1_macro'),\n",
    "            f1_micro=comp_metrics.get('f1_micro'),\n",
    "            precision_macro=comp_metrics.get('precision_macro'),\n",
    "            recall_macro=comp_metrics.get('recall_macro'),\n",
    "            f1_per_class=comp_metrics.get('f1_per_class')\n",
    "        )\n",
    "        \n",
    "        # Store per-class metrics if this is the best epoch\n",
    "        if val_acc >= metrics.best_accuracy and class_names is not None:\n",
    "            metrics.per_class_f1 = comp_metrics.get('per_class_f1')\n",
    "            metrics.per_class_precision = comp_metrics.get('per_class_precision')\n",
    "            metrics.per_class_recall = comp_metrics.get('per_class_recall')\n",
    "            metrics.best_confusion_matrix = comp_metrics.get('confusion_matrix')\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"\\nEpoch {epoch+1} Summary:\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "        print(f\"Learning Rate: {current_lr:.2e}\")\n",
    "        print(f\"Best Val Accuracy: {metrics.best_accuracy:.4f}\")\n",
    "        print(f\"Trainable Parameters: {model.get_trainable_parameters_count():,}\")\n",
    "        \n",
    "        # Save checkpoint\n",
    "        is_best = val_acc >= metrics.best_accuracy\n",
    "        checkpoint_path = save_checkpoint(model, optimizer, lr_scheduler, epoch, \n",
    "                                        metrics, is_best, checkpoint_dir)\n",
    "        \n",
    "        # Plot learning curves every 5 epochs\n",
    "        if epoch % 5 == 0:\n",
    "            plot_path = os.path.join(checkpoint_dir, f'learning_curves_epoch_{epoch+1:03d}.png')\n",
    "            plot_learning_curves(epoch + 1, metrics, plot_path)\n",
    "        \n",
    "        # Early stopping check\n",
    "        early_stopper(val_loss, model)\n",
    "        if early_stopper.early_stop:\n",
    "            print(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Step learning rate scheduler\n",
    "        if lr_scheduler:\n",
    "            lr_scheduler.step()\n",
    "        \n",
    "        # Clear GPU cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n Training completed!\")\n",
    "    print(f\"Best validation accuracy: {metrics.best_accuracy:.4f}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5114e30b-f8e6-45a4-b1fb-2a83e3850307",
   "metadata": {},
   "source": [
    "### train_and_evaluate_model\n",
    "- Since we're testing individual models it will be useful to have a wrapper function to train a model that we can iterate\n",
    "- Create a model-specific directory to store the latest and best performing iterations of the model\n",
    "- Create the dataloaders using the create_dataloaders function (described above)\n",
    "- Set up the loss function, optimizer, and learning rate scheduler\n",
    "- Call the training function (described above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e72b7753-ecf1-4ef1-962b-0cfd09daa281",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model_name, base_config, device):\n",
    "\n",
    "    # Pull the seed from the config\n",
    "    seed = base_config.get('training', {}).get('seed', 1234)\n",
    "    \n",
    "    # Set seed before anything else\n",
    "    seed_all(seed)\n",
    "\n",
    "    # Decorative formatting for the model training\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING MODEL: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    config = copy.deepcopy(base_config)\n",
    "    \n",
    "    # Create model-specific checkpoint directory\n",
    "    base_checkpoint_dir = base_config['training']['checkpoint_dir']\n",
    "    model_checkpoint_dir = os.path.join(base_checkpoint_dir, model_name)\n",
    "    config['training']['checkpoint_dir'] = model_checkpoint_dir\n",
    "    config['model']['model_name'] = model_name\n",
    "    \n",
    "    # Create the directory\n",
    "    os.makedirs(model_checkpoint_dir, exist_ok=True)\n",
    "    print(f\"Checkpoint directory: {model_checkpoint_dir}\")\n",
    "    \n",
    "    # Create model\n",
    "    model = ImageClassificationModel(config)\n",
    "    print(f\"Model created: {model_name}\")\n",
    "    print(f\"Number of classes: {config['model']['num_classes']}\")\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
    "        base_dir=config['data']['base_dir'],\n",
    "        batch_size=config['training']['batch_size'],\n",
    "        num_workers=2, \n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # Training setup\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # Trainable parameters are those not frozen (â‰  requires_grad)\n",
    "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
    "    # Set the optimizer  and learning rate scheduler \n",
    "    optimizer = optim.Adam(trainable_params, lr=config['training']['learning_rate'])\n",
    "    scheduler = lr_scheduler.StepLR(\n",
    "        optimizer, \n",
    "        step_size=config['training']['scheduler_step_size'],\n",
    "        gamma=config['training']['scheduler_gamma']\n",
    "    )\n",
    "    \n",
    "    # Updated training config to include class names for AUROC calculation\n",
    "    training_config = {\n",
    "        'device': device,\n",
    "        'model': model,\n",
    "        'criterion': criterion,\n",
    "        'optimizer': optimizer,\n",
    "        'lr_scheduler': scheduler,\n",
    "        'train_dataloader': train_loader, \n",
    "        'val_dataloader': val_loader,\n",
    "        'num_epochs': config['training']['num_epochs'],\n",
    "        'checkpoint_dir': model_checkpoint_dir,\n",
    "        'epoch_start_unfreeze': config['training']['epoch_start_unfreeze'],\n",
    "        'layer_start_unfreeze': config['training']['layer_start_unfreeze'],\n",
    "        'early_stopping_patience': config['training']['early_stopping_patience'],\n",
    "        'class_names': class_names, \n",
    "        'seed':seed\n",
    "    }\n",
    "    \n",
    "    # Train model\n",
    "    metrics = train_model(**training_config)\n",
    "\n",
    "    # Added to run resulting model on the test dir alongside the training/validation - doesn't integrate with the plotting function for confusion matrices/etc. yet\n",
    "    if test_loader is not None:\n",
    "        print(\"\\nEvaluating on test set...\")\n",
    "        _, test_acc, test_comp_metrics = validate_epoch_with_metrics(\n",
    "            model, test_loader, criterion, device, class_names\n",
    "        )\n",
    "        print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "        print(f\"Test AUROC (Macro): {test_comp_metrics.get('auroc_macro', 0):.4f}\")\n",
    "        print(f\"Test F1 (Macro): {test_comp_metrics.get('f1_macro', 0):.4f}\")\n",
    "        \n",
    "        # Store test metrics\n",
    "        test_results = {\n",
    "            'accuracy': test_acc,\n",
    "            'auroc_macro': test_comp_metrics.get('auroc_macro'),\n",
    "            'f1_macro': test_comp_metrics.get('f1_macro'),\n",
    "            'confusion_matrix': test_comp_metrics.get('confusion_matrix')\n",
    "        }\n",
    "    else:\n",
    "        test_results = None\n",
    "    \n",
    "    # Plot comprehensive metrics at the end\n",
    "    if class_names is not None:\n",
    "        plot_metrics(metrics, class_names, os.path.join(model_checkpoint_dir, 'all_metrics.png'))\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'metrics': metrics,\n",
    "        'test_results': test_results,\n",
    "        'config': config,\n",
    "        'model': model,\n",
    "        'checkpoint_dir': model_checkpoint_dir,\n",
    "        'class_names': class_names, \n",
    "        'seed':seed\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8853c80-440e-4188-98bc-e38139085e44",
   "metadata": {},
   "source": [
    "### run_model_comparison\n",
    "- Helper function to run the individual models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "28f71b77-0b23-4a5c-95ef-9786a1d3f011",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_model_comparison(models_to_try, base_config, device, experiment_name=None):\n",
    "\n",
    "    # Get and set the seed \n",
    "    seed = base_config.get('training', {}).get('seed', 1234)\n",
    "    seed_all(seed)\n",
    "\n",
    "    # Make a directory for the experiment name that will serve as a parent directory for the individual trained models \n",
    "    if experiment_name:\n",
    "        # Add experiment name to base directory\n",
    "        base_config['training']['checkpoint_dir'] = f\"./checkpoints/{experiment_name}\"\n",
    "        print(f\"Experiment directory: {base_config['training']['checkpoint_dir']}\")\n",
    "    \n",
    "    os.makedirs(base_config['training']['checkpoint_dir'], exist_ok=True)\n",
    "    \n",
    "    # Instantiate a dict to hold all the results \n",
    "    all_results = {}\n",
    "\n",
    "    # Iterate the training/evaluation function over the models in the models_to_try list \n",
    "    for i, model_name in enumerate(models_to_try, 1):\n",
    "        print(f\"\\n[{i}/{len(models_to_try)}] Training {model_name}...\")\n",
    "        \n",
    "        try:\n",
    "            result = train_and_evaluate_model(model_name, base_config, device)\n",
    "            all_results[model_name] = result\n",
    "            \n",
    "            # Display summary\n",
    "            metrics = result['metrics']\n",
    "            print(f\"{model_name}: Best Val Acc = {metrics.best_accuracy:.4f}\")\n",
    "            print(f\"Checkpoints saved to: {result['checkpoint_dir']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"{model_name} failed: {str(e)}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    \n",
    "    # Return the dict of the results \n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdc9900-611b-4be5-a52f-593902595767",
   "metadata": {},
   "source": [
    "## Iterate through the different models on the brain tumor dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb524d6-9f44-4013-9091-afb09154bd8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment directory: ./checkpoints/brain_tumor_v1\n",
      "\n",
      "[1/24] Training convnext_tiny...\n",
      "\n",
      "============================================================\n",
      "TRAINING MODEL: convnext_tiny\n",
      "============================================================\n",
      "Checkpoint directory: ./checkpoints/brain_tumor_v1/convnext_tiny\n",
      "Freezing layers of pretrained model\n",
      "Model created: convnext_tiny\n",
      "Number of classes: 4\n",
      "Seed val: 1234\n",
      "Starting training...\n",
      "\n",
      "============================================================\n",
      "Epoch 1/100\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  15%|â–ˆâ–        | 20/136 [01:00<05:08,  2.66s/it, Loss=0.3975, Acc=0.7812]"
     ]
    }
   ],
   "source": [
    "# Make a list of all of the models stored in our dictionary\n",
    "all_models = [\n",
    "    'convnext_tiny',\n",
    "    'convnext_small',\n",
    "    'convnext_base',\n",
    "    'convnext_large',\n",
    "    'resnet18',\n",
    "    'resnet34',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'resnet152',\n",
    "    'efficientnet_b0',\n",
    "    'efficientnet_b1',\n",
    "    'efficientnet_b2',\n",
    "    'efficientnet_b3',\n",
    "    'efficientnet_b4',\n",
    "    'efficientnet_b5',\n",
    "    'efficientnet_b6',\n",
    "    'efficientnet_b7',\n",
    "    'efficientnet_v2_s',\n",
    "    'efficientnet_v2_m',\n",
    "    'efficientnet_v2_l',\n",
    "    'densenet121',\n",
    "    'densenet161',\n",
    "    'densenet169',\n",
    "    'densenet201'\n",
    "]\n",
    "# Set the device as GPU if we're on a GPU node, CPU if not\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Set the base_config\n",
    "base_config = {\n",
    "    \"model\": {\n",
    "        \"num_classes\": 4,\n",
    "        \"model_name\": \"placeholder\", \n",
    "        \"n_nodes\": 512,\n",
    "        \"dropout\": 0.2,\n",
    "        \"freeze_layers\": True,\n",
    "        \"freeze_strategy\": \"all\",\n",
    "        \"pretrained\": True,\n",
    "        \"custom_head\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"size\": 224,\n",
    "        \"base_dir\": \"./mri_modeling/brain_tumor_data/\",\n",
    "        \"seed\": 1234\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 100,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_step_size\": 15,\n",
    "        \"scheduler_gamma\": 0.5,\n",
    "        \"checkpoint_dir\": \"./checkpoints\",\n",
    "        \"epoch_start_unfreeze\": 5,\n",
    "        \"layer_start_unfreeze\": 140,\n",
    "        \"early_stopping_patience\": 10, \n",
    "        \"seed\": 1234\n",
    "    },\n",
    "    \"debug\": False\n",
    "}\n",
    "results = run_model_comparison(all_models, base_config, device, experiment_name=\"brain_tumor_v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d304ba51-ab5d-47d8-b2bc-9859bfdbba3e",
   "metadata": {},
   "source": [
    "### evaluate_model_on_test\n",
    "- Function that will be deprecated when refactored b/c testing was incorporated into the training function\n",
    "- Used to run the trained models on the test set and return metrics for plotting and visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b63403b-a2b4-44b2-a75c-90c7f87c136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the trained models on the testing directory\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "def evaluate_model_on_test(model, test_loader, device, model_name, class_names=None):\n",
    "\n",
    "    # Create lists to hold the predictions, labels, and probabilities to save during evaluation \n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    all_probabilities = []\n",
    "    test_loss = 0.0\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Decorate formatting for the evaluation print output\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Testing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # No backpropogation\n",
    "    with torch.no_grad():\n",
    "        # Iterate through batches from the dataloader for the test set, wrapping it in tqdm to have a nice progress bar\n",
    "        # All of this will be stored until the end and moved to CPU for numpy calculations \n",
    "        for batch_idx, (inputs, labels) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            # More the image tensors and labels to the GPU\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            # Compute the loss and add product of the average loss * batch size to the running total \n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Get predictions and convert to probabilities using softmax\n",
    "            probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            \n",
    "            # Store results and move from the GPU to the CPU\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "    \n",
    "    # Calculate metrics for loss and accuracy \n",
    "    test_loss = test_loss / len(test_loader.dataset)\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    \n",
    "    # Classification report\n",
    "    if class_names is None:\n",
    "        class_names = [f\"Class_{i}\" for i in range(len(np.unique(all_labels)))]\n",
    "    \n",
    "    report = classification_report(\n",
    "        all_labels, \n",
    "        all_predictions, \n",
    "        target_names=class_names,\n",
    "        output_dict=True\n",
    "    )\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_predictions)\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    \n",
    "    # Create results dictionary\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'test_accuracy': accuracy,\n",
    "        'test_loss': test_loss,\n",
    "        'classification_report': report,\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': np.array(all_predictions),\n",
    "        'true_labels': np.array(all_labels),\n",
    "        'probabilities': np.array(all_probabilities),\n",
    "        'per_class_accuracy': dict(zip(class_names, per_class_accuracy)),\n",
    "        'total_samples': len(all_labels)\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\nResults - {model_name}\")\n",
    "    print(f\"   Test Loss: {test_loss:.4f}\")\n",
    "    print(f\"   Test Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Per-class Accuracy:\")\n",
    "    for class_name, acc in results['per_class_accuracy'].items():\n",
    "        print(f\"     {class_name}: {acc:.4f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05a66b30-ec8f-4cb8-ae41-93e978d1f6bb",
   "metadata": {},
   "source": [
    "### evaluate_all_models and save_model_results \n",
    "- Helper function to iterate through all of the models and run them on the test set and save the plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6965bc6-1376-4c44-925c-6f66901af7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_all_models(experiment_dir, base_config, device, test_loader, class_names=None):\n",
    "    import glob\n",
    "\n",
    "    # Find all model directories\n",
    "    model_dirs = [d for d in os.listdir(experiment_dir) \n",
    "                 if os.path.isdir(os.path.join(experiment_dir, d))]\n",
    "\n",
    "    # Print the number of models found \n",
    "    print(f\"Found {len(model_dirs)} models in {experiment_dir}\")\n",
    "\n",
    "    # Instantiate a dictionary to hold all results and a list to hold the summary data\n",
    "    all_results = {}\n",
    "    summary_data = []\n",
    "\n",
    "    # Iterate through the models, loading the best one (best.pth) and then evaulating its performance on the test set \n",
    "    for model_name in model_dirs:\n",
    "        model_dir = os.path.join(experiment_dir, model_name)\n",
    "        checkpoint_path = os.path.join(model_dir, 'best.pth')\n",
    "        \n",
    "        if not os.path.exists(checkpoint_path):\n",
    "            print(f\"No best checkpoint for {model_name}, trying latest.pth\")\n",
    "            checkpoint_path = os.path.join(model_dir, 'latest.pth')\n",
    "        \n",
    "        if os.path.exists(checkpoint_path):\n",
    "            try:\n",
    "                print(f\"\\nLoading {model_name}...\")\n",
    "                \n",
    "                # Load checkpoint\n",
    "                checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "                \n",
    "                # Update config with model name\n",
    "                model_config = base_config.copy()\n",
    "                model_config['model']['model_name'] = model_name\n",
    "                \n",
    "                # Create model\n",
    "                model = ImageClassificationModel(model_config)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                model = model.to(device)\n",
    "                \n",
    "                # Evaluate on test set\n",
    "                results = evaluate_model_on_test(\n",
    "                    model, test_loader, device, model_name, class_names\n",
    "                )\n",
    "                \n",
    "                all_results[model_name] = results\n",
    "                \n",
    "                # Add to summary\n",
    "                summary_data.append({\n",
    "                    'Model': model_name,\n",
    "                    'Test Accuracy': results['test_accuracy'],\n",
    "                    'Test Loss': results['test_loss'],\n",
    "                    'Training Accuracy': checkpoint.get('metrics', {}).get('best_accuracy', 'N/A'),\n",
    "                    'Training Loss': checkpoint.get('training_info', {}).get('best_loss', 'N/A'),\n",
    "                    'Checkpoint Path': checkpoint_path\n",
    "                })\n",
    "                \n",
    "                # Save individual model results\n",
    "                save_model_results(results, model_dir)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Failed to evaluate {model_name}: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "        else:\n",
    "            print(f\"No checkpoint found for {model_name}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    summary_df = summary_df.sort_values('Test Accuracy', ascending=False)\n",
    "    \n",
    "    return all_results, summary_df\n",
    "\n",
    "def save_model_results(results, save_dir):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    # Save metrics as JSON\n",
    "    import json\n",
    "    metrics_to_save = {\n",
    "        'model_name': results['model_name'],\n",
    "        'test_accuracy': results['test_accuracy'],\n",
    "        'test_loss': results['test_loss'],\n",
    "        'per_class_accuracy': results['per_class_accuracy'],\n",
    "        'total_samples': results['total_samples']\n",
    "    }\n",
    "    \n",
    "    with open(os.path.join(save_dir, 'test_results.json'), 'w') as f:\n",
    "        json.dump(metrics_to_save, f, indent=2)\n",
    "    \n",
    "    # Save predictions as CSV\n",
    "    predictions_df = pd.DataFrame({\n",
    "        'true_label': results['true_labels'],\n",
    "        'predicted_label': results['predictions'],\n",
    "        'max_probability': np.max(results['probabilities'], axis=1)\n",
    "    })\n",
    "    \n",
    "    # Add per-class probabilities\n",
    "    for i in range(results['probabilities'].shape[1]):\n",
    "        predictions_df[f'prob_class_{i}'] = results['probabilities'][:, i]\n",
    "    \n",
    "    predictions_df.to_csv(os.path.join(save_dir, 'test_predictions.csv'), index=False)\n",
    "    \n",
    "    print(f\"Results saved to {save_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaacc254-cf82-4ec9-a959-6dc94715028f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "experiment_dir = \"./checkpoints/brain_tumor_v1\"\n",
    "\n",
    "base_config = {\n",
    "    \"model\": {\n",
    "        \"num_classes\": 4,\n",
    "        \"model_name\": \"placeholder\",\n",
    "        \"n_nodes\": 512,\n",
    "        \"dropout\": 0.2,\n",
    "        \"freeze_layers\": False,\n",
    "        \"freeze_strategy\": \"all\",\n",
    "        \"pretrained\": True,\n",
    "        \"custom_head\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"size\": 224,\n",
    "        \"base_dir\": \"./mri_modeling/brain_tumor_data/\",\n",
    "    },\n",
    "    \"debug\": False\n",
    "}\n",
    "\n",
    "train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
    "    base_dir=base_config['data']['base_dir'],  # Use the base_dir from config\n",
    "    batch_size=32,  # Specify batch size\n",
    "    num_workers=2    # Specify workers\n",
    ")\n",
    "\n",
    "# Get class names from the training dataset\n",
    "train_dataset = ImageFolder(os.path.join(base_config['data']['base_dir'], 'train'))\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\nEvaluating all trained models...\")\n",
    "all_results, summary_df = evaluate_all_models(\n",
    "    experiment_dir=experiment_dir,\n",
    "    base_config=base_config,\n",
    "    device=device,\n",
    "    test_loader=test_loader,\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1557700-696a-4418-936d-ca61cba71b69",
   "metadata": {},
   "source": [
    "### plot_confusion_matrices\n",
    "- Function to plot confusion matrices for the performance of individual classes for each model/misclassification \n",
    "### plot_model_comparison\n",
    "- Function to plot the test accuracy, test loss, and per-class accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5cb10e-407a-4498-b865-72f8b5142fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrices(all_results, class_names, save_dir=None):\n",
    "    # Pull the number of models and then make a grid based on the number of models used\n",
    "    n_models = len(all_results)\n",
    "    n_cols = min(3, n_models)\n",
    "    n_rows = (n_models + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "    axes = axes.flatten() if n_models > 1 else [axes]\n",
    "    \n",
    "    for idx, (model_name, results) in enumerate(all_results.items()):\n",
    "        ax = axes[idx]\n",
    "        cm = results['confusion_matrix']\n",
    "        \n",
    "        # Normalize confusion matrix\n",
    "        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        \n",
    "        sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                   xticklabels=class_names, yticklabels=class_names,\n",
    "                   ax=ax, cbar_kws={'label': 'Normalized Count'})\n",
    "        \n",
    "        ax.set_title(f'{model_name}\\nAccuracy: {results[\"test_accuracy\"]:.3f}')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for idx in range(len(all_results), len(axes)):\n",
    "        axes[idx].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def plot_model_comparison(all_results, save_dir=None):\n",
    "    # Prepare data\n",
    "    model_names = list(all_results.keys())\n",
    "    test_accuracies = [all_results[name]['test_accuracy'] for name in model_names]\n",
    "    test_losses = [all_results[name]['test_loss'] for name in model_names]\n",
    "    \n",
    "    # Sort by accuracy\n",
    "    sorted_indices = np.argsort(test_accuracies)[::-1]\n",
    "    model_names = [model_names[i] for i in sorted_indices]\n",
    "    test_accuracies = [test_accuracies[i] for i in sorted_indices]\n",
    "    test_losses = [test_losses[i] for i in sorted_indices]\n",
    "    \n",
    "    # Create figure\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    # Define x-axis positions for the ticks to quiet the MatPlotLib warning \n",
    "    x_pos = np.arange(len(model_names))\n",
    "    \n",
    "    # Plot 1: Test Accuracy\n",
    "    bars1 = ax1.bar(x_pos, test_accuracies, color='skyblue', alpha=0.7) \n",
    "    ax1.set_title('Test Accuracy Comparison')\n",
    "    ax1.set_ylabel('Accuracy')\n",
    "    ax1.set_xticks(x_pos)\n",
    "    ax1.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    ax1.set_ylim([0, 1])\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, acc in zip(bars1, test_accuracies):\n",
    "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{acc:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    # Plot 2: Test Loss\n",
    "    bars2 = ax2.bar(x_pos, test_losses, color='lightcoral', alpha=0.7) \n",
    "    ax2.set_title('Test Loss Comparison')\n",
    "    ax2.set_ylabel('Loss')\n",
    "    ax2.set_xticks(x_pos)\n",
    "    ax2.set_xticklabels(model_names, rotation=45, ha='right')\n",
    "    \n",
    "    # Plot 3: Per-class accuracy heatmap\n",
    "    per_class_data = []\n",
    "    for model_name in model_names:\n",
    "        per_class_acc = list(all_results[model_name]['per_class_accuracy'].values())\n",
    "        per_class_data.append(per_class_acc)\n",
    "    \n",
    "    if per_class_data:\n",
    "        per_class_data = np.array(per_class_data)\n",
    "        im = ax3.imshow(per_class_data, cmap='YlOrRd', aspect='auto', vmin=0.8, vmax=1)\n",
    "        ax3.set_title('Per-Class Accuracy Heatmap')\n",
    "        ax3.set_xlabel('Class')\n",
    "        ax3.set_ylabel('Model')\n",
    "        ax3.set_xticks(range(len(class_names)))\n",
    "        ax3.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "        ax3.set_yticks(range(len(model_names)))\n",
    "        ax3.set_yticklabels(model_names)\n",
    "        \n",
    "        # Add colorbar\n",
    "        plt.colorbar(im, ax=ax3, label='Accuracy')\n",
    "        \n",
    "        # Add text annotations\n",
    "        for i in range(len(model_names)):\n",
    "            for j in range(len(class_names)):\n",
    "                ax3.text(j, i, f'{per_class_data[i, j]:.2f}',\n",
    "                        ha='center', va='center', color='black', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_dir:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        plt.savefig(os.path.join(save_dir, 'model_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0483b73-9c6c-4c99-ae3a-320ff47cf4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(all_results, class_names, save_dir=experiment_dir)\n",
    "plot_model_comparison(all_results, save_dir=experiment_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c682ed05-266b-49ab-90a3-445ac40676d1",
   "metadata": {},
   "source": [
    "Now, let's try on a different condition - Alzheimer's disease. I don't anticipate that this will work as well because there is a class imbalance issue with the data being used to train (very few moderately demented images). When I've run it, it usually has errors for the first few epochs `(UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use zero_division parameter to control this behavior`.) due to the moderately demented images not being observed or classified, but this local minima is overcome through more epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed25f8-24b0-4549-b125-53aa194497f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "base_config = {\n",
    "    \"model\": {\n",
    "        \"num_classes\": 4,\n",
    "        \"model_name\": \"placeholder\", \n",
    "        \"n_nodes\": 512,\n",
    "        \"dropout\": 0.2,\n",
    "        \"freeze_layers\": True,\n",
    "        \"freeze_strategy\": \"all\",\n",
    "        \"pretrained\": True,\n",
    "        \"custom_head\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"size\": 224,\n",
    "        \"base_dir\": \"./mri_modeling/alzheimers_data/\",\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"batch_size\": 32,\n",
    "        \"num_epochs\": 100,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_step_size\": 15,\n",
    "        \"scheduler_gamma\": 0.1,\n",
    "        \"checkpoint_dir\": \"./checkpoints\",\n",
    "        \"epoch_start_unfreeze\": 5,\n",
    "        \"layer_start_unfreeze\": 140,\n",
    "        \"early_stopping_patience\": 10\n",
    "    },\n",
    "    \"debug\": False\n",
    "}\n",
    "\n",
    "results_alz = run_model_comparison(all_models, base_config, device, experiment_name=\"alzheimers_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935dca93-17e7-4ff1-9e0f-44c578025ae8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment_dir = \"./checkpoints/alzheimers_v1\"\n",
    "\n",
    "base_config = {\n",
    "    \"model\": {\n",
    "        \"num_classes\": 4,\n",
    "        \"model_name\": \"placeholder\",\n",
    "        \"n_nodes\": 512,\n",
    "        \"dropout\": 0.2,\n",
    "        \"freeze_layers\": False,\n",
    "        \"freeze_strategy\": \"all\",\n",
    "        \"pretrained\": True,\n",
    "        \"custom_head\": True\n",
    "    },\n",
    "    \"data\": {\n",
    "        \"size\": 224,\n",
    "        \"base_dir\": \"./mri_modeling/alzheimers_data/\",\n",
    "    },\n",
    "    \"debug\": False\n",
    "}\n",
    "\n",
    "train_loader, val_loader, test_loader, class_names = create_dataloaders(\n",
    "    base_dir=base_config['data']['base_dir'], \n",
    "    batch_size=32, \n",
    "    num_workers=2 \n",
    ")\n",
    "\n",
    "# Get class names from the training dataset\n",
    "train_dataset = ImageFolder(os.path.join(base_config['data']['base_dir'], 'train'))\n",
    "class_names = train_dataset.classes\n",
    "print(f\"Classes: {class_names}\")\n",
    "print(f\"Test samples: {len(test_loader.dataset)}\")\n",
    "\n",
    "# Evaluate all models\n",
    "print(\"\\nEvaluating all trained models...\")\n",
    "all_results, summary_df = evaluate_all_models(\n",
    "    experiment_dir=experiment_dir,\n",
    "    base_config=base_config,\n",
    "    device=device,\n",
    "    test_loader=test_loader,\n",
    "    class_names=class_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712c04fd-4cd8-4321-8cb4-77f04730b180",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrices(all_results, class_names, save_dir=experiment_dir)\n",
    "plot_model_comparison(all_results, save_dir=experiment_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
